{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "*********************\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print('*********************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.329177\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "# print(loss)\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 3.264999 analytic: -0.185317, relative error: 1.000000e+00\n",
      "numerical: 1.330532 analytic: -1.723067, relative error: 1.000000e+00\n",
      "numerical: 3.006747 analytic: -0.412728, relative error: 1.000000e+00\n",
      "numerical: 0.409568 analytic: -2.996029, relative error: 1.000000e+00\n",
      "numerical: 1.692830 analytic: -1.155999, relative error: 1.000000e+00\n",
      "numerical: -2.449803 analytic: 1.100654, relative error: 1.000000e+00\n",
      "numerical: 0.404728 analytic: -0.648627, relative error: 1.000000e+00\n",
      "numerical: -1.023242 analytic: 0.468930, relative error: 1.000000e+00\n",
      "numerical: 0.793310 analytic: -1.514088, relative error: 1.000000e+00\n",
      "numerical: 0.816291 analytic: 0.473981, relative error: 2.653006e-01\n",
      "numerical: 2.271299 analytic: -2.105917, relative error: 1.000000e+00\n",
      "numerical: -1.341453 analytic: 0.173064, relative error: 1.000000e+00\n",
      "numerical: 1.456473 analytic: -2.195696, relative error: 1.000000e+00\n",
      "numerical: -2.592360 analytic: 2.996688, relative error: 1.000000e+00\n",
      "numerical: -1.922853 analytic: 3.255805, relative error: 1.000000e+00\n",
      "numerical: -2.796600 analytic: 0.472145, relative error: 1.000000e+00\n",
      "numerical: 1.636922 analytic: -1.563768, relative error: 1.000000e+00\n",
      "numerical: -3.165045 analytic: 1.376480, relative error: 1.000000e+00\n",
      "numerical: -1.497752 analytic: 0.434511, relative error: 1.000000e+00\n",
      "numerical: -2.996371 analytic: 1.099246, relative error: 1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.329177e+00 computed in 0.026045s\n",
      "vectorized loss: 2.329177e+00 computed in 0.017880s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 390.750594\n",
      "iteration 100 / 2000: loss 237.319916\n",
      "iteration 200 / 2000: loss 144.541236\n",
      "iteration 300 / 2000: loss 88.868097\n",
      "iteration 400 / 2000: loss 54.840935\n",
      "iteration 500 / 2000: loss 34.535505\n",
      "iteration 600 / 2000: loss 22.056539\n",
      "iteration 700 / 2000: loss 14.700415\n",
      "iteration 800 / 2000: loss 10.121979\n",
      "iteration 900 / 2000: loss 7.387137\n",
      "iteration 1000 / 2000: loss 5.793280\n",
      "iteration 1100 / 2000: loss 4.676866\n",
      "iteration 1200 / 2000: loss 4.132803\n",
      "iteration 1300 / 2000: loss 3.810136\n",
      "iteration 1400 / 2000: loss 3.637812\n",
      "iteration 1500 / 2000: loss 3.357497\n",
      "iteration 1600 / 2000: loss 3.367393\n",
      "iteration 1700 / 2000: loss 3.369642\n",
      "iteration 1800 / 2000: loss 3.317664\n",
      "iteration 1900 / 2000: loss 3.352076\n",
      "iteration 0 / 2000: loss 467.835609\n",
      "iteration 100 / 2000: loss 256.784386\n",
      "iteration 200 / 2000: loss 141.631038\n",
      "iteration 300 / 2000: loss 78.878067\n",
      "iteration 400 / 2000: loss 44.445938\n",
      "iteration 500 / 2000: loss 25.833148\n",
      "iteration 600 / 2000: loss 15.508635\n",
      "iteration 700 / 2000: loss 9.963814\n",
      "iteration 800 / 2000: loss 6.834886\n",
      "iteration 900 / 2000: loss 5.158051\n",
      "iteration 1000 / 2000: loss 4.248705\n",
      "iteration 1100 / 2000: loss 3.770492\n",
      "iteration 1200 / 2000: loss 3.514451\n",
      "iteration 1300 / 2000: loss 3.371846\n",
      "iteration 1400 / 2000: loss 3.274209\n",
      "iteration 1500 / 2000: loss 3.168445\n",
      "iteration 1600 / 2000: loss 3.159060\n",
      "iteration 1700 / 2000: loss 3.232497\n",
      "iteration 1800 / 2000: loss 3.108483\n",
      "iteration 1900 / 2000: loss 3.159355\n",
      "iteration 0 / 2000: loss 535.603639\n",
      "iteration 100 / 2000: loss 265.956261\n",
      "iteration 200 / 2000: loss 133.290163\n",
      "iteration 300 / 2000: loss 67.449472\n",
      "iteration 400 / 2000: loss 34.913715\n",
      "iteration 500 / 2000: loss 18.810058\n",
      "iteration 600 / 2000: loss 10.880451\n",
      "iteration 700 / 2000: loss 6.976084\n",
      "iteration 800 / 2000: loss 5.145869\n",
      "iteration 900 / 2000: loss 4.047706\n",
      "iteration 1000 / 2000: loss 3.491550\n",
      "iteration 1100 / 2000: loss 3.318193\n",
      "iteration 1200 / 2000: loss 3.281202\n",
      "iteration 1300 / 2000: loss 3.176628\n",
      "iteration 1400 / 2000: loss 3.150591\n",
      "iteration 1500 / 2000: loss 3.092613\n",
      "iteration 1600 / 2000: loss 3.018065\n",
      "iteration 1700 / 2000: loss 3.124829\n",
      "iteration 1800 / 2000: loss 3.185448\n",
      "iteration 1900 / 2000: loss 3.110588\n",
      "iteration 0 / 2000: loss 617.736490\n",
      "iteration 100 / 2000: loss 277.934754\n",
      "iteration 200 / 2000: loss 126.055866\n",
      "iteration 300 / 2000: loss 58.057076\n",
      "iteration 400 / 2000: loss 27.725113\n",
      "iteration 500 / 2000: loss 14.057838\n",
      "iteration 600 / 2000: loss 7.969226\n",
      "iteration 700 / 2000: loss 5.212427\n",
      "iteration 800 / 2000: loss 4.008639\n",
      "iteration 900 / 2000: loss 3.534541\n",
      "iteration 1000 / 2000: loss 3.233498\n",
      "iteration 1100 / 2000: loss 3.138911\n",
      "iteration 1200 / 2000: loss 3.124413\n",
      "iteration 1300 / 2000: loss 3.095556\n",
      "iteration 1400 / 2000: loss 3.102157\n",
      "iteration 1500 / 2000: loss 2.992463\n",
      "iteration 1600 / 2000: loss 3.020359\n",
      "iteration 1700 / 2000: loss 3.107532\n",
      "iteration 1800 / 2000: loss 3.060903\n",
      "iteration 1900 / 2000: loss 3.082209\n",
      "iteration 0 / 2000: loss 695.481212\n",
      "iteration 100 / 2000: loss 282.857329\n",
      "iteration 200 / 2000: loss 116.353812\n",
      "iteration 300 / 2000: loss 48.875721\n",
      "iteration 400 / 2000: loss 21.669623\n",
      "iteration 500 / 2000: loss 10.553055\n",
      "iteration 600 / 2000: loss 6.060173\n",
      "iteration 700 / 2000: loss 4.176977\n",
      "iteration 800 / 2000: loss 3.486709\n",
      "iteration 900 / 2000: loss 3.259082\n",
      "iteration 1000 / 2000: loss 3.052060\n",
      "iteration 1100 / 2000: loss 3.074632\n",
      "iteration 1200 / 2000: loss 3.125885\n",
      "iteration 1300 / 2000: loss 2.960858\n",
      "iteration 1400 / 2000: loss 2.989234\n",
      "iteration 1500 / 2000: loss 3.061149\n",
      "iteration 1600 / 2000: loss 3.022585\n",
      "iteration 1700 / 2000: loss 3.012981\n",
      "iteration 1800 / 2000: loss 3.050042\n",
      "iteration 1900 / 2000: loss 2.919762\n",
      "iteration 0 / 2000: loss 772.989851\n",
      "iteration 100 / 2000: loss 284.804227\n",
      "iteration 200 / 2000: loss 106.018711\n",
      "iteration 300 / 2000: loss 40.814810\n",
      "iteration 400 / 2000: loss 16.705761\n",
      "iteration 500 / 2000: loss 8.069372\n",
      "iteration 600 / 2000: loss 4.813660\n",
      "iteration 700 / 2000: loss 3.680034\n",
      "iteration 800 / 2000: loss 3.214236\n",
      "iteration 900 / 2000: loss 3.083722\n",
      "iteration 1000 / 2000: loss 2.998493\n",
      "iteration 1100 / 2000: loss 2.969762\n",
      "iteration 1200 / 2000: loss 2.941837\n",
      "iteration 1300 / 2000: loss 2.960085\n",
      "iteration 1400 / 2000: loss 2.897891\n",
      "iteration 1500 / 2000: loss 3.044430\n",
      "iteration 1600 / 2000: loss 3.001264\n",
      "iteration 1700 / 2000: loss 2.931955\n",
      "iteration 1800 / 2000: loss 3.048395\n",
      "iteration 1900 / 2000: loss 2.909727\n",
      "iteration 0 / 2000: loss 388.457061\n",
      "iteration 100 / 2000: loss 143.998812\n",
      "iteration 200 / 2000: loss 54.483532\n",
      "iteration 300 / 2000: loss 22.063091\n",
      "iteration 400 / 2000: loss 10.021765\n",
      "iteration 500 / 2000: loss 5.688683\n",
      "iteration 600 / 2000: loss 4.109575\n",
      "iteration 700 / 2000: loss 3.596359\n",
      "iteration 800 / 2000: loss 3.408089\n",
      "iteration 900 / 2000: loss 3.189420\n",
      "iteration 1000 / 2000: loss 3.210315\n",
      "iteration 1100 / 2000: loss 3.267052\n",
      "iteration 1200 / 2000: loss 3.182289\n",
      "iteration 1300 / 2000: loss 3.230393\n",
      "iteration 1400 / 2000: loss 3.246262\n",
      "iteration 1500 / 2000: loss 3.279643\n",
      "iteration 1600 / 2000: loss 3.195718\n",
      "iteration 1700 / 2000: loss 3.271794\n",
      "iteration 1800 / 2000: loss 3.244675\n",
      "iteration 1900 / 2000: loss 3.226367\n",
      "iteration 0 / 2000: loss 461.955720\n",
      "iteration 100 / 2000: loss 140.066801\n",
      "iteration 200 / 2000: loss 44.102082\n",
      "iteration 300 / 2000: loss 15.318677\n",
      "iteration 400 / 2000: loss 6.666743\n",
      "iteration 500 / 2000: loss 4.256932\n",
      "iteration 600 / 2000: loss 3.421918\n",
      "iteration 700 / 2000: loss 3.256435\n",
      "iteration 800 / 2000: loss 3.193513\n",
      "iteration 900 / 2000: loss 3.129310\n",
      "iteration 1000 / 2000: loss 3.146842\n",
      "iteration 1100 / 2000: loss 3.186857\n",
      "iteration 1200 / 2000: loss 3.164183\n",
      "iteration 1300 / 2000: loss 3.171833\n",
      "iteration 1400 / 2000: loss 3.156117\n",
      "iteration 1500 / 2000: loss 3.132160\n",
      "iteration 1600 / 2000: loss 3.294552\n",
      "iteration 1700 / 2000: loss 3.140771\n",
      "iteration 1800 / 2000: loss 3.140035\n",
      "iteration 1900 / 2000: loss 3.138880\n",
      "iteration 0 / 2000: loss 544.480154\n",
      "iteration 100 / 2000: loss 135.228598\n",
      "iteration 200 / 2000: loss 35.261613\n",
      "iteration 300 / 2000: loss 10.978358\n",
      "iteration 400 / 2000: loss 5.034192\n",
      "iteration 500 / 2000: loss 3.619227\n",
      "iteration 600 / 2000: loss 3.214586\n",
      "iteration 700 / 2000: loss 3.129680\n",
      "iteration 800 / 2000: loss 3.080198\n",
      "iteration 900 / 2000: loss 3.107736\n",
      "iteration 1000 / 2000: loss 3.212092\n",
      "iteration 1100 / 2000: loss 3.087784\n",
      "iteration 1200 / 2000: loss 3.117737\n",
      "iteration 1300 / 2000: loss 3.096508\n",
      "iteration 1400 / 2000: loss 3.113126\n",
      "iteration 1500 / 2000: loss 3.150059\n",
      "iteration 1600 / 2000: loss 3.063399\n",
      "iteration 1700 / 2000: loss 3.034856\n",
      "iteration 1800 / 2000: loss 3.064966\n",
      "iteration 1900 / 2000: loss 2.984724\n",
      "iteration 0 / 2000: loss 625.741605\n",
      "iteration 100 / 2000: loss 127.009281\n",
      "iteration 200 / 2000: loss 27.866246\n",
      "iteration 300 / 2000: loss 7.982980\n",
      "iteration 400 / 2000: loss 4.017548\n",
      "iteration 500 / 2000: loss 3.249143\n",
      "iteration 600 / 2000: loss 3.138920\n",
      "iteration 700 / 2000: loss 3.011566\n",
      "iteration 800 / 2000: loss 3.147538\n",
      "iteration 900 / 2000: loss 3.059757\n",
      "iteration 1000 / 2000: loss 3.095894\n",
      "iteration 1100 / 2000: loss 3.061876\n",
      "iteration 1200 / 2000: loss 3.067761\n",
      "iteration 1300 / 2000: loss 3.093080\n",
      "iteration 1400 / 2000: loss 3.063783\n",
      "iteration 1500 / 2000: loss 3.100192\n",
      "iteration 1600 / 2000: loss 3.051022\n",
      "iteration 1700 / 2000: loss 3.007160\n",
      "iteration 1800 / 2000: loss 3.066369\n",
      "iteration 1900 / 2000: loss 3.108103\n",
      "iteration 0 / 2000: loss 699.906464\n",
      "iteration 100 / 2000: loss 116.700835\n",
      "iteration 200 / 2000: loss 21.514919\n",
      "iteration 300 / 2000: loss 6.065369\n",
      "iteration 400 / 2000: loss 3.512379\n",
      "iteration 500 / 2000: loss 3.125619\n",
      "iteration 600 / 2000: loss 2.948477\n",
      "iteration 700 / 2000: loss 2.963460\n",
      "iteration 800 / 2000: loss 3.038913\n",
      "iteration 900 / 2000: loss 3.053842\n",
      "iteration 1000 / 2000: loss 3.025522\n",
      "iteration 1100 / 2000: loss 2.996088\n",
      "iteration 1200 / 2000: loss 3.025796\n",
      "iteration 1300 / 2000: loss 3.040231\n",
      "iteration 1400 / 2000: loss 3.023502\n",
      "iteration 1500 / 2000: loss 3.020942\n",
      "iteration 1600 / 2000: loss 3.001649\n",
      "iteration 1700 / 2000: loss 3.048702\n",
      "iteration 1800 / 2000: loss 3.016504\n",
      "iteration 1900 / 2000: loss 3.024862\n",
      "iteration 0 / 2000: loss 781.348844\n",
      "iteration 100 / 2000: loss 106.698794\n",
      "iteration 200 / 2000: loss 16.845121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 2000: loss 4.828434\n",
      "iteration 400 / 2000: loss 3.185936\n",
      "iteration 500 / 2000: loss 2.991321\n",
      "iteration 600 / 2000: loss 3.056051\n",
      "iteration 700 / 2000: loss 2.958916\n",
      "iteration 800 / 2000: loss 3.020757\n",
      "iteration 900 / 2000: loss 2.966650\n",
      "iteration 1000 / 2000: loss 2.918552\n",
      "iteration 1100 / 2000: loss 2.922286\n",
      "iteration 1200 / 2000: loss 2.981565\n",
      "iteration 1300 / 2000: loss 2.965427\n",
      "iteration 1400 / 2000: loss 2.989454\n",
      "iteration 1500 / 2000: loss 2.891069\n",
      "iteration 1600 / 2000: loss 2.935593\n",
      "iteration 1700 / 2000: loss 2.855131\n",
      "iteration 1800 / 2000: loss 3.013072\n",
      "iteration 1900 / 2000: loss 2.954985\n",
      "iteration 0 / 2000: loss 383.891189\n",
      "iteration 100 / 2000: loss 87.013240\n",
      "iteration 200 / 2000: loss 21.673895\n",
      "iteration 300 / 2000: loss 7.219775\n",
      "iteration 400 / 2000: loss 4.063630\n",
      "iteration 500 / 2000: loss 3.485976\n",
      "iteration 600 / 2000: loss 3.390437\n",
      "iteration 700 / 2000: loss 3.272340\n",
      "iteration 800 / 2000: loss 3.115469\n",
      "iteration 900 / 2000: loss 3.290142\n",
      "iteration 1000 / 2000: loss 3.196106\n",
      "iteration 1100 / 2000: loss 3.064743\n",
      "iteration 1200 / 2000: loss 3.300169\n",
      "iteration 1300 / 2000: loss 3.201771\n",
      "iteration 1400 / 2000: loss 3.184816\n",
      "iteration 1500 / 2000: loss 3.235056\n",
      "iteration 1600 / 2000: loss 3.182613\n",
      "iteration 1700 / 2000: loss 3.259025\n",
      "iteration 1800 / 2000: loss 3.211138\n",
      "iteration 1900 / 2000: loss 3.146695\n",
      "iteration 0 / 2000: loss 470.575826\n",
      "iteration 100 / 2000: loss 79.076286\n",
      "iteration 200 / 2000: loss 15.518843\n",
      "iteration 300 / 2000: loss 5.172890\n",
      "iteration 400 / 2000: loss 3.490521\n",
      "iteration 500 / 2000: loss 3.127356\n",
      "iteration 600 / 2000: loss 3.178904\n",
      "iteration 700 / 2000: loss 3.158895\n",
      "iteration 800 / 2000: loss 3.191022\n",
      "iteration 900 / 2000: loss 3.083066\n",
      "iteration 1000 / 2000: loss 3.046303\n",
      "iteration 1100 / 2000: loss 3.178125\n",
      "iteration 1200 / 2000: loss 3.136886\n",
      "iteration 1300 / 2000: loss 3.134280\n",
      "iteration 1400 / 2000: loss 3.238076\n",
      "iteration 1500 / 2000: loss 3.136980\n",
      "iteration 1600 / 2000: loss 3.143816\n",
      "iteration 1700 / 2000: loss 3.059197\n",
      "iteration 1800 / 2000: loss 3.152732\n",
      "iteration 1900 / 2000: loss 3.140151\n",
      "iteration 0 / 2000: loss 548.528698\n",
      "iteration 100 / 2000: loss 68.554095\n",
      "iteration 200 / 2000: loss 11.002603\n",
      "iteration 300 / 2000: loss 3.959143\n",
      "iteration 400 / 2000: loss 3.247869\n",
      "iteration 500 / 2000: loss 3.145722\n",
      "iteration 600 / 2000: loss 3.170141\n",
      "iteration 700 / 2000: loss 3.059122\n",
      "iteration 800 / 2000: loss 3.119603\n",
      "iteration 900 / 2000: loss 2.989387\n",
      "iteration 1000 / 2000: loss 3.065325\n",
      "iteration 1100 / 2000: loss 3.155039\n",
      "iteration 1200 / 2000: loss 3.114963\n",
      "iteration 1300 / 2000: loss 3.109294\n",
      "iteration 1400 / 2000: loss 3.076553\n",
      "iteration 1500 / 2000: loss 3.046036\n",
      "iteration 1600 / 2000: loss 3.033926\n",
      "iteration 1700 / 2000: loss 3.088789\n",
      "iteration 1800 / 2000: loss 3.038719\n",
      "iteration 1900 / 2000: loss 3.092491\n",
      "iteration 0 / 2000: loss 627.612458\n",
      "iteration 100 / 2000: loss 58.629700\n",
      "iteration 200 / 2000: loss 7.940276\n",
      "iteration 300 / 2000: loss 3.466459\n",
      "iteration 400 / 2000: loss 3.130928\n",
      "iteration 500 / 2000: loss 2.988334\n",
      "iteration 600 / 2000: loss 3.022873\n",
      "iteration 700 / 2000: loss 3.130076\n",
      "iteration 800 / 2000: loss 2.974809\n",
      "iteration 900 / 2000: loss 3.077556\n",
      "iteration 1000 / 2000: loss 3.064980\n",
      "iteration 1100 / 2000: loss 3.070474\n",
      "iteration 1200 / 2000: loss 3.093973\n",
      "iteration 1300 / 2000: loss 2.981769\n",
      "iteration 1400 / 2000: loss 2.963777\n",
      "iteration 1500 / 2000: loss 3.073466\n",
      "iteration 1600 / 2000: loss 3.007025\n",
      "iteration 1700 / 2000: loss 3.061999\n",
      "iteration 1800 / 2000: loss 3.025718\n",
      "iteration 1900 / 2000: loss 3.118713\n",
      "iteration 0 / 2000: loss 695.035659\n",
      "iteration 100 / 2000: loss 48.243028\n",
      "iteration 200 / 2000: loss 5.998859\n",
      "iteration 300 / 2000: loss 3.117846\n",
      "iteration 400 / 2000: loss 3.017705\n",
      "iteration 500 / 2000: loss 3.024246\n",
      "iteration 600 / 2000: loss 2.975634\n",
      "iteration 700 / 2000: loss 2.967766\n",
      "iteration 800 / 2000: loss 3.018372\n",
      "iteration 900 / 2000: loss 3.017665\n",
      "iteration 1000 / 2000: loss 3.007334\n",
      "iteration 1100 / 2000: loss 3.012682\n",
      "iteration 1200 / 2000: loss 2.968364\n",
      "iteration 1300 / 2000: loss 3.037622\n",
      "iteration 1400 / 2000: loss 2.993047\n",
      "iteration 1500 / 2000: loss 3.001339\n",
      "iteration 1600 / 2000: loss 3.047807\n",
      "iteration 1700 / 2000: loss 2.963224\n",
      "iteration 1800 / 2000: loss 2.958687\n",
      "iteration 1900 / 2000: loss 3.019016\n",
      "iteration 0 / 2000: loss 778.462003\n",
      "iteration 100 / 2000: loss 40.403337\n",
      "iteration 200 / 2000: loss 4.761351\n",
      "iteration 300 / 2000: loss 3.012089\n",
      "iteration 400 / 2000: loss 2.938145\n",
      "iteration 500 / 2000: loss 2.974855\n",
      "iteration 600 / 2000: loss 3.007611\n",
      "iteration 700 / 2000: loss 2.933296\n",
      "iteration 800 / 2000: loss 2.925958\n",
      "iteration 900 / 2000: loss 2.955858\n",
      "iteration 1000 / 2000: loss 2.946524\n",
      "iteration 1100 / 2000: loss 2.850833\n",
      "iteration 1200 / 2000: loss 2.981691\n",
      "iteration 1300 / 2000: loss 2.981237\n",
      "iteration 1400 / 2000: loss 2.961864\n",
      "iteration 1500 / 2000: loss 2.980323\n",
      "iteration 1600 / 2000: loss 2.984239\n",
      "iteration 1700 / 2000: loss 3.028992\n",
      "iteration 1800 / 2000: loss 2.965879\n",
      "iteration 1900 / 2000: loss 2.993613\n",
      "iteration 0 / 2000: loss 394.358352\n",
      "iteration 100 / 2000: loss 55.262080\n",
      "iteration 200 / 2000: loss 10.190681\n",
      "iteration 300 / 2000: loss 4.146478\n",
      "iteration 400 / 2000: loss 3.285605\n",
      "iteration 500 / 2000: loss 3.226570\n",
      "iteration 600 / 2000: loss 3.262631\n",
      "iteration 700 / 2000: loss 3.119170\n",
      "iteration 800 / 2000: loss 3.135225\n",
      "iteration 900 / 2000: loss 3.149469\n",
      "iteration 1000 / 2000: loss 3.200867\n",
      "iteration 1100 / 2000: loss 3.211349\n",
      "iteration 1200 / 2000: loss 3.263854\n",
      "iteration 1300 / 2000: loss 3.246357\n",
      "iteration 1400 / 2000: loss 3.186629\n",
      "iteration 1500 / 2000: loss 3.254752\n",
      "iteration 1600 / 2000: loss 3.226649\n",
      "iteration 1700 / 2000: loss 3.152520\n",
      "iteration 1800 / 2000: loss 3.316374\n",
      "iteration 1900 / 2000: loss 3.191337\n",
      "iteration 0 / 2000: loss 462.680924\n",
      "iteration 100 / 2000: loss 43.891816\n",
      "iteration 200 / 2000: loss 6.832626\n",
      "iteration 300 / 2000: loss 3.430538\n",
      "iteration 400 / 2000: loss 3.209423\n",
      "iteration 500 / 2000: loss 3.246620\n",
      "iteration 600 / 2000: loss 3.140414\n",
      "iteration 700 / 2000: loss 3.053894\n",
      "iteration 800 / 2000: loss 3.107436\n",
      "iteration 900 / 2000: loss 3.144927\n",
      "iteration 1000 / 2000: loss 3.226588\n",
      "iteration 1100 / 2000: loss 3.128461\n",
      "iteration 1200 / 2000: loss 3.178251\n",
      "iteration 1300 / 2000: loss 2.993123\n",
      "iteration 1400 / 2000: loss 3.041617\n",
      "iteration 1500 / 2000: loss 3.095943\n",
      "iteration 1600 / 2000: loss 3.251786\n",
      "iteration 1700 / 2000: loss 3.227680\n",
      "iteration 1800 / 2000: loss 3.223705\n",
      "iteration 1900 / 2000: loss 3.082877\n",
      "iteration 0 / 2000: loss 555.799669\n",
      "iteration 100 / 2000: loss 35.588866\n",
      "iteration 200 / 2000: loss 5.094876\n",
      "iteration 300 / 2000: loss 3.271739\n",
      "iteration 400 / 2000: loss 3.110223\n",
      "iteration 500 / 2000: loss 3.171829\n",
      "iteration 600 / 2000: loss 3.030600\n",
      "iteration 700 / 2000: loss 3.110646\n",
      "iteration 800 / 2000: loss 3.055277\n",
      "iteration 900 / 2000: loss 3.043017\n",
      "iteration 1000 / 2000: loss 3.125742\n",
      "iteration 1100 / 2000: loss 3.126327\n",
      "iteration 1200 / 2000: loss 3.036330\n",
      "iteration 1300 / 2000: loss 3.090721\n",
      "iteration 1400 / 2000: loss 3.096690\n",
      "iteration 1500 / 2000: loss 3.093462\n",
      "iteration 1600 / 2000: loss 3.126311\n",
      "iteration 1700 / 2000: loss 3.037761\n",
      "iteration 1800 / 2000: loss 3.119325\n",
      "iteration 1900 / 2000: loss 3.166196\n",
      "iteration 0 / 2000: loss 620.171861\n",
      "iteration 100 / 2000: loss 27.308729\n",
      "iteration 200 / 2000: loss 3.995893\n",
      "iteration 300 / 2000: loss 3.058632\n",
      "iteration 400 / 2000: loss 3.052287\n",
      "iteration 500 / 2000: loss 3.138928\n",
      "iteration 600 / 2000: loss 3.058411\n",
      "iteration 700 / 2000: loss 2.955750\n",
      "iteration 800 / 2000: loss 3.017428\n",
      "iteration 900 / 2000: loss 3.141574\n",
      "iteration 1000 / 2000: loss 3.051906\n",
      "iteration 1100 / 2000: loss 3.035590\n",
      "iteration 1200 / 2000: loss 3.087671\n",
      "iteration 1300 / 2000: loss 3.103969\n",
      "iteration 1400 / 2000: loss 3.018901\n",
      "iteration 1500 / 2000: loss 3.039326\n",
      "iteration 1600 / 2000: loss 3.072728\n",
      "iteration 1700 / 2000: loss 3.069148\n",
      "iteration 1800 / 2000: loss 2.987388\n",
      "iteration 1900 / 2000: loss 2.992917\n",
      "iteration 0 / 2000: loss 694.157666\n",
      "iteration 100 / 2000: loss 20.954107\n",
      "iteration 200 / 2000: loss 3.430281\n",
      "iteration 300 / 2000: loss 2.987875\n",
      "iteration 400 / 2000: loss 2.963555\n",
      "iteration 500 / 2000: loss 2.964111\n",
      "iteration 600 / 2000: loss 3.044311\n",
      "iteration 700 / 2000: loss 3.043822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 2000: loss 3.044222\n",
      "iteration 900 / 2000: loss 3.019354\n",
      "iteration 1000 / 2000: loss 3.033268\n",
      "iteration 1100 / 2000: loss 2.923905\n",
      "iteration 1200 / 2000: loss 3.065131\n",
      "iteration 1300 / 2000: loss 2.945971\n",
      "iteration 1400 / 2000: loss 3.027102\n",
      "iteration 1500 / 2000: loss 2.991015\n",
      "iteration 1600 / 2000: loss 2.994409\n",
      "iteration 1700 / 2000: loss 3.046390\n",
      "iteration 1800 / 2000: loss 3.064807\n",
      "iteration 1900 / 2000: loss 2.978108\n",
      "iteration 0 / 2000: loss 771.650920\n",
      "iteration 100 / 2000: loss 16.393903\n",
      "iteration 200 / 2000: loss 3.152124\n",
      "iteration 300 / 2000: loss 3.007757\n",
      "iteration 400 / 2000: loss 2.950861\n",
      "iteration 500 / 2000: loss 2.980216\n",
      "iteration 600 / 2000: loss 2.935690\n",
      "iteration 700 / 2000: loss 3.006990\n",
      "iteration 800 / 2000: loss 2.865487\n",
      "iteration 900 / 2000: loss 2.937074\n",
      "iteration 1000 / 2000: loss 2.991317\n",
      "iteration 1100 / 2000: loss 2.937663\n",
      "iteration 1200 / 2000: loss 2.990231\n",
      "iteration 1300 / 2000: loss 2.950132\n",
      "iteration 1400 / 2000: loss 2.975408\n",
      "iteration 1500 / 2000: loss 2.954117\n",
      "iteration 1600 / 2000: loss 2.993991\n",
      "iteration 1700 / 2000: loss 2.996782\n",
      "iteration 1800 / 2000: loss 2.937451\n",
      "iteration 1900 / 2000: loss 2.929259\n",
      "iteration 0 / 2000: loss 389.954492\n",
      "iteration 100 / 2000: loss 34.069882\n",
      "iteration 200 / 2000: loss 5.664694\n",
      "iteration 300 / 2000: loss 3.475885\n",
      "iteration 400 / 2000: loss 3.428910\n",
      "iteration 500 / 2000: loss 3.325924\n",
      "iteration 600 / 2000: loss 3.106208\n",
      "iteration 700 / 2000: loss 3.371824\n",
      "iteration 800 / 2000: loss 3.087281\n",
      "iteration 900 / 2000: loss 3.256767\n",
      "iteration 1000 / 2000: loss 3.131957\n",
      "iteration 1100 / 2000: loss 3.213310\n",
      "iteration 1200 / 2000: loss 3.253354\n",
      "iteration 1300 / 2000: loss 3.215959\n",
      "iteration 1400 / 2000: loss 3.261122\n",
      "iteration 1500 / 2000: loss 3.262074\n",
      "iteration 1600 / 2000: loss 3.243915\n",
      "iteration 1700 / 2000: loss 3.174185\n",
      "iteration 1800 / 2000: loss 3.107004\n",
      "iteration 1900 / 2000: loss 3.279013\n",
      "iteration 0 / 2000: loss 461.354910\n",
      "iteration 100 / 2000: loss 25.190234\n",
      "iteration 200 / 2000: loss 4.211346\n",
      "iteration 300 / 2000: loss 3.273719\n",
      "iteration 400 / 2000: loss 3.214230\n",
      "iteration 500 / 2000: loss 3.169388\n",
      "iteration 600 / 2000: loss 3.149557\n",
      "iteration 700 / 2000: loss 3.199255\n",
      "iteration 800 / 2000: loss 3.175728\n",
      "iteration 900 / 2000: loss 3.155064\n",
      "iteration 1000 / 2000: loss 3.193142\n",
      "iteration 1100 / 2000: loss 3.273092\n",
      "iteration 1200 / 2000: loss 3.129026\n",
      "iteration 1300 / 2000: loss 3.138150\n",
      "iteration 1400 / 2000: loss 3.144595\n",
      "iteration 1500 / 2000: loss 3.204769\n",
      "iteration 1600 / 2000: loss 3.169511\n",
      "iteration 1700 / 2000: loss 3.148342\n",
      "iteration 1800 / 2000: loss 3.165552\n",
      "iteration 1900 / 2000: loss 3.188852\n",
      "iteration 0 / 2000: loss 544.634671\n",
      "iteration 100 / 2000: loss 18.788144\n",
      "iteration 200 / 2000: loss 3.581143\n",
      "iteration 300 / 2000: loss 3.170878\n",
      "iteration 400 / 2000: loss 3.248352\n",
      "iteration 500 / 2000: loss 3.088501\n",
      "iteration 600 / 2000: loss 3.050969\n",
      "iteration 700 / 2000: loss 3.029061\n",
      "iteration 800 / 2000: loss 3.052528\n",
      "iteration 900 / 2000: loss 3.150578\n",
      "iteration 1000 / 2000: loss 3.091942\n",
      "iteration 1100 / 2000: loss 3.011216\n",
      "iteration 1200 / 2000: loss 3.167797\n",
      "iteration 1300 / 2000: loss 3.096459\n",
      "iteration 1400 / 2000: loss 3.109517\n",
      "iteration 1500 / 2000: loss 3.036875\n",
      "iteration 1600 / 2000: loss 3.106241\n",
      "iteration 1700 / 2000: loss 3.143135\n",
      "iteration 1800 / 2000: loss 3.051875\n",
      "iteration 1900 / 2000: loss 3.082639\n",
      "iteration 0 / 2000: loss 617.168788\n",
      "iteration 100 / 2000: loss 13.544611\n",
      "iteration 200 / 2000: loss 3.253258\n",
      "iteration 300 / 2000: loss 3.134344\n",
      "iteration 400 / 2000: loss 3.068296\n",
      "iteration 500 / 2000: loss 3.090545\n",
      "iteration 600 / 2000: loss 3.054257\n",
      "iteration 700 / 2000: loss 3.025021\n",
      "iteration 800 / 2000: loss 3.028138\n",
      "iteration 900 / 2000: loss 3.032204\n",
      "iteration 1000 / 2000: loss 3.025715\n",
      "iteration 1100 / 2000: loss 3.032757\n",
      "iteration 1200 / 2000: loss 2.974044\n",
      "iteration 1300 / 2000: loss 2.926769\n",
      "iteration 1400 / 2000: loss 3.076010\n",
      "iteration 1500 / 2000: loss 3.091605\n",
      "iteration 1600 / 2000: loss 2.955091\n",
      "iteration 1700 / 2000: loss 3.024353\n",
      "iteration 1800 / 2000: loss 3.108274\n",
      "iteration 1900 / 2000: loss 2.997771\n",
      "iteration 0 / 2000: loss 698.258776\n",
      "iteration 100 / 2000: loss 10.288802\n",
      "iteration 200 / 2000: loss 3.066602\n",
      "iteration 300 / 2000: loss 3.112754\n",
      "iteration 400 / 2000: loss 3.034670\n",
      "iteration 500 / 2000: loss 3.068931\n",
      "iteration 600 / 2000: loss 3.067915\n",
      "iteration 700 / 2000: loss 2.949791\n",
      "iteration 800 / 2000: loss 2.985048\n",
      "iteration 900 / 2000: loss 3.041740\n",
      "iteration 1000 / 2000: loss 3.116941\n",
      "iteration 1100 / 2000: loss 3.025926\n",
      "iteration 1200 / 2000: loss 2.982933\n",
      "iteration 1300 / 2000: loss 3.094694\n",
      "iteration 1400 / 2000: loss 3.097465\n",
      "iteration 1500 / 2000: loss 3.083877\n",
      "iteration 1600 / 2000: loss 3.032051\n",
      "iteration 1700 / 2000: loss 3.042262\n",
      "iteration 1800 / 2000: loss 2.932336\n",
      "iteration 1900 / 2000: loss 2.986184\n",
      "iteration 0 / 2000: loss 781.817438\n",
      "iteration 100 / 2000: loss 7.823634\n",
      "iteration 200 / 2000: loss 3.005021\n",
      "iteration 300 / 2000: loss 3.018305\n",
      "iteration 400 / 2000: loss 2.922807\n",
      "iteration 500 / 2000: loss 3.036357\n",
      "iteration 600 / 2000: loss 2.981463\n",
      "iteration 700 / 2000: loss 2.910571\n",
      "iteration 800 / 2000: loss 2.928997\n",
      "iteration 900 / 2000: loss 2.992506\n",
      "iteration 1000 / 2000: loss 2.959816\n",
      "iteration 1100 / 2000: loss 2.988825\n",
      "iteration 1200 / 2000: loss 2.939367\n",
      "iteration 1300 / 2000: loss 2.907939\n",
      "iteration 1400 / 2000: loss 2.939758\n",
      "iteration 1500 / 2000: loss 2.981692\n",
      "iteration 1600 / 2000: loss 2.984652\n",
      "iteration 1700 / 2000: loss 2.925489\n",
      "iteration 1800 / 2000: loss 2.974705\n",
      "iteration 1900 / 2000: loss 3.009881\n",
      "iteration 0 / 2000: loss 384.194779\n",
      "iteration 100 / 2000: loss 21.618888\n",
      "iteration 200 / 2000: loss 4.085019\n",
      "iteration 300 / 2000: loss 3.224116\n",
      "iteration 400 / 2000: loss 3.261091\n",
      "iteration 500 / 2000: loss 3.185066\n",
      "iteration 600 / 2000: loss 3.195522\n",
      "iteration 700 / 2000: loss 3.233715\n",
      "iteration 800 / 2000: loss 3.273521\n",
      "iteration 900 / 2000: loss 3.198182\n",
      "iteration 1000 / 2000: loss 3.246872\n",
      "iteration 1100 / 2000: loss 3.213563\n",
      "iteration 1200 / 2000: loss 3.158846\n",
      "iteration 1300 / 2000: loss 3.199014\n",
      "iteration 1400 / 2000: loss 3.124712\n",
      "iteration 1500 / 2000: loss 3.266595\n",
      "iteration 1600 / 2000: loss 3.265685\n",
      "iteration 1700 / 2000: loss 3.211992\n",
      "iteration 1800 / 2000: loss 3.233809\n",
      "iteration 1900 / 2000: loss 3.165971\n",
      "iteration 0 / 2000: loss 461.065540\n",
      "iteration 100 / 2000: loss 15.029085\n",
      "iteration 200 / 2000: loss 3.593420\n",
      "iteration 300 / 2000: loss 3.096050\n",
      "iteration 400 / 2000: loss 3.036861\n",
      "iteration 500 / 2000: loss 3.166814\n",
      "iteration 600 / 2000: loss 3.141971\n",
      "iteration 700 / 2000: loss 3.309305\n",
      "iteration 800 / 2000: loss 3.202737\n",
      "iteration 900 / 2000: loss 3.228760\n",
      "iteration 1000 / 2000: loss 3.120212\n",
      "iteration 1100 / 2000: loss 3.132883\n",
      "iteration 1200 / 2000: loss 3.111640\n",
      "iteration 1300 / 2000: loss 3.101043\n",
      "iteration 1400 / 2000: loss 3.141238\n",
      "iteration 1500 / 2000: loss 3.213422\n",
      "iteration 1600 / 2000: loss 3.283921\n",
      "iteration 1700 / 2000: loss 3.121163\n",
      "iteration 1800 / 2000: loss 3.095391\n",
      "iteration 1900 / 2000: loss 3.150105\n",
      "iteration 0 / 2000: loss 534.630301\n",
      "iteration 100 / 2000: loss 10.644587\n",
      "iteration 200 / 2000: loss 3.341415\n",
      "iteration 300 / 2000: loss 3.127658\n",
      "iteration 400 / 2000: loss 3.122778\n",
      "iteration 500 / 2000: loss 3.142458\n",
      "iteration 600 / 2000: loss 3.132813\n",
      "iteration 700 / 2000: loss 3.138936\n",
      "iteration 800 / 2000: loss 3.200905\n",
      "iteration 900 / 2000: loss 3.115428\n",
      "iteration 1000 / 2000: loss 3.069767\n",
      "iteration 1100 / 2000: loss 3.168195\n",
      "iteration 1200 / 2000: loss 3.042271\n",
      "iteration 1300 / 2000: loss 3.087288\n",
      "iteration 1400 / 2000: loss 3.065192\n",
      "iteration 1500 / 2000: loss 3.134622\n",
      "iteration 1600 / 2000: loss 3.046576\n",
      "iteration 1700 / 2000: loss 3.134085\n",
      "iteration 1800 / 2000: loss 3.135728\n",
      "iteration 1900 / 2000: loss 3.113699\n",
      "iteration 0 / 2000: loss 620.037195\n",
      "iteration 100 / 2000: loss 7.821147\n",
      "iteration 200 / 2000: loss 3.174827\n",
      "iteration 300 / 2000: loss 3.007672\n",
      "iteration 400 / 2000: loss 3.121690\n",
      "iteration 500 / 2000: loss 3.036463\n",
      "iteration 600 / 2000: loss 3.050137\n",
      "iteration 700 / 2000: loss 3.056693\n",
      "iteration 800 / 2000: loss 3.125447\n",
      "iteration 900 / 2000: loss 3.059154\n",
      "iteration 1000 / 2000: loss 3.090841\n",
      "iteration 1100 / 2000: loss 3.118652\n",
      "iteration 1200 / 2000: loss 3.031190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 2000: loss 2.964436\n",
      "iteration 1400 / 2000: loss 3.003125\n",
      "iteration 1500 / 2000: loss 3.019926\n",
      "iteration 1600 / 2000: loss 3.073924\n",
      "iteration 1700 / 2000: loss 3.059469\n",
      "iteration 1800 / 2000: loss 2.996241\n",
      "iteration 1900 / 2000: loss 2.955312\n",
      "iteration 0 / 2000: loss 695.294459\n",
      "iteration 100 / 2000: loss 5.863510\n",
      "iteration 200 / 2000: loss 3.006691\n",
      "iteration 300 / 2000: loss 3.043212\n",
      "iteration 400 / 2000: loss 3.125617\n",
      "iteration 500 / 2000: loss 2.979654\n",
      "iteration 600 / 2000: loss 3.049808\n",
      "iteration 700 / 2000: loss 3.095221\n",
      "iteration 800 / 2000: loss 2.999974\n",
      "iteration 900 / 2000: loss 2.986535\n",
      "iteration 1000 / 2000: loss 3.113520\n",
      "iteration 1100 / 2000: loss 3.022521\n",
      "iteration 1200 / 2000: loss 2.991722\n",
      "iteration 1300 / 2000: loss 3.014986\n",
      "iteration 1400 / 2000: loss 3.010027\n",
      "iteration 1500 / 2000: loss 3.024212\n",
      "iteration 1600 / 2000: loss 3.083075\n",
      "iteration 1700 / 2000: loss 2.968678\n",
      "iteration 1800 / 2000: loss 3.085858\n",
      "iteration 1900 / 2000: loss 2.999218\n",
      "iteration 0 / 2000: loss 773.691488\n",
      "iteration 100 / 2000: loss 4.681821\n",
      "iteration 200 / 2000: loss 2.918991\n",
      "iteration 300 / 2000: loss 2.921661\n",
      "iteration 400 / 2000: loss 3.057920\n",
      "iteration 500 / 2000: loss 2.953537\n",
      "iteration 600 / 2000: loss 3.108577\n",
      "iteration 700 / 2000: loss 3.057608\n",
      "iteration 800 / 2000: loss 2.907359\n",
      "iteration 900 / 2000: loss 3.017954\n",
      "iteration 1000 / 2000: loss 2.961026\n",
      "iteration 1100 / 2000: loss 2.987088\n",
      "iteration 1200 / 2000: loss 3.004931\n",
      "iteration 1300 / 2000: loss 3.015244\n",
      "iteration 1400 / 2000: loss 2.931902\n",
      "iteration 1500 / 2000: loss 3.042116\n",
      "iteration 1600 / 2000: loss 3.042889\n",
      "iteration 1700 / 2000: loss 3.005708\n",
      "iteration 1800 / 2000: loss 2.994323\n",
      "iteration 1900 / 2000: loss 3.005227\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.025306 val accuracy: 0.031000\n",
      "lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.027061 val accuracy: 0.030000\n",
      "lr 1.000000e-07 reg 3.500000e+04 train accuracy: 0.026939 val accuracy: 0.029000\n",
      "lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.026735 val accuracy: 0.030000\n",
      "lr 1.000000e-07 reg 4.500000e+04 train accuracy: 0.027286 val accuracy: 0.032000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.027163 val accuracy: 0.030000\n",
      "lr 2.000000e-07 reg 2.500000e+04 train accuracy: 0.026980 val accuracy: 0.033000\n",
      "lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.026510 val accuracy: 0.032000\n",
      "lr 2.000000e-07 reg 3.500000e+04 train accuracy: 0.027163 val accuracy: 0.030000\n",
      "lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.027143 val accuracy: 0.030000\n",
      "lr 2.000000e-07 reg 4.500000e+04 train accuracy: 0.027429 val accuracy: 0.031000\n",
      "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.028082 val accuracy: 0.032000\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.025673 val accuracy: 0.031000\n",
      "lr 3.000000e-07 reg 3.000000e+04 train accuracy: 0.026551 val accuracy: 0.031000\n",
      "lr 3.000000e-07 reg 3.500000e+04 train accuracy: 0.027082 val accuracy: 0.031000\n",
      "lr 3.000000e-07 reg 4.000000e+04 train accuracy: 0.027388 val accuracy: 0.032000\n",
      "lr 3.000000e-07 reg 4.500000e+04 train accuracy: 0.026490 val accuracy: 0.032000\n",
      "lr 3.000000e-07 reg 5.000000e+04 train accuracy: 0.027184 val accuracy: 0.030000\n",
      "lr 4.000000e-07 reg 2.500000e+04 train accuracy: 0.026041 val accuracy: 0.033000\n",
      "lr 4.000000e-07 reg 3.000000e+04 train accuracy: 0.027041 val accuracy: 0.034000\n",
      "lr 4.000000e-07 reg 3.500000e+04 train accuracy: 0.027531 val accuracy: 0.036000\n",
      "lr 4.000000e-07 reg 4.000000e+04 train accuracy: 0.028796 val accuracy: 0.034000\n",
      "lr 4.000000e-07 reg 4.500000e+04 train accuracy: 0.027265 val accuracy: 0.030000\n",
      "lr 4.000000e-07 reg 5.000000e+04 train accuracy: 0.028061 val accuracy: 0.034000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.026816 val accuracy: 0.032000\n",
      "lr 5.000000e-07 reg 3.000000e+04 train accuracy: 0.026490 val accuracy: 0.030000\n",
      "lr 5.000000e-07 reg 3.500000e+04 train accuracy: 0.027776 val accuracy: 0.033000\n",
      "lr 5.000000e-07 reg 4.000000e+04 train accuracy: 0.027714 val accuracy: 0.035000\n",
      "lr 5.000000e-07 reg 4.500000e+04 train accuracy: 0.029408 val accuracy: 0.032000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.026673 val accuracy: 0.032000\n",
      "lr 6.000000e-07 reg 2.500000e+04 train accuracy: 0.025653 val accuracy: 0.033000\n",
      "lr 6.000000e-07 reg 3.000000e+04 train accuracy: 0.025796 val accuracy: 0.035000\n",
      "lr 6.000000e-07 reg 3.500000e+04 train accuracy: 0.026551 val accuracy: 0.032000\n",
      "lr 6.000000e-07 reg 4.000000e+04 train accuracy: 0.026612 val accuracy: 0.033000\n",
      "lr 6.000000e-07 reg 4.500000e+04 train accuracy: 0.029102 val accuracy: 0.031000\n",
      "lr 6.000000e-07 reg 5.000000e+04 train accuracy: 0.028388 val accuracy: 0.035000\n",
      "best validation accuracy achieved during cross-validation: 0.036000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7,2e-7,3e-7,4e-7,5e-7, 6e-7]\n",
    "regularization_strengths = [2.5e4,3e4,3.5e4,4e4,4.5e4,5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "ite=2000\n",
    "\n",
    "for i in learning_rates:\n",
    "    for j in regularization_strengths:\n",
    "        soft = Softmax()\n",
    "        soft.train(X_train,y_train,learning_rate=i,reg=j,num_iters=ite,verbose=True)\n",
    "        y_predict = soft.predict(X_train)\n",
    "        train_acc = np.mean(y_predict==y_train)\n",
    "        y_predict = soft.predict(X_val)\n",
    "        pred_acc = np.mean(y_predict==y_val)\n",
    "        results[(i,j)] = (train_acc,pred_acc)\n",
    "        if pred_acc>best_val:\n",
    "            best_val=pred_acc\n",
    "            best_softmax=soft\n",
    "        \n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "# retrain with 2000 iters for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.029000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb96fd83630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
